                                                                  Building of Psuedocode
                                                                  
                                                                  
Huffman’s algorithm requires first determining the probability (or frequency of occurrence, or count) of each of the possible N symbols as 
they occur in the information source to be coded.For example, suppose:the possible items are the symbols A,B,C,D,E,F,G,H (so N=8)
and the message sequence to be coded is AAAAABBAHHBCBGCCC (so K=17).

1st Step: Determine Frequencies:
Using a completely filled trie, we could construct a code for the possible symbols that would use log 2 (8) * 17 = 51 bits to represent this sequence
But we can do better! The table of counts or frequencies for this message would be:
A: 6; B: 4; C: 4; D: 0; E: 0; F: 0; G: 1; H: 2

 2nd Step:Construction of  the optimum coding trie:
 The basic idea is:
We want common items to be near the root, so they have a short code; rare items can be farther from the root.
In fact, we want an optimum trie: one that gives the smallest average code length.
We could attempt to build the trie from the top down. For example:
A in this example is the most common item... make it a child of the root.
B,C are the next most common items... put them farther down the tree.
G,H are the rarest items... make them farthest from the root.

But while pretty good, this coding trie is not optimum, and it’s not easy to come up with an efficient algorithm that will build the optimum coding trie top-down
Huffman’s breakthrough was instead to build the trie from the bottom up, and he proved that his approach is guaranteed to produce an optimum coding trie.

3rd step:Huffman’s algorithm: constructing the tree

Once you have the frequencies for all the possible items in the sequence to be coded, Huffman’s algorithm will construct an optimal binary trie for coding
The algorithm starts with a “forest” of single-node trees, one for each symbol in the input alphabet
At each step of the algorithm, two trees T1, T2 in the “forest” (which two, do you think?) are joined into one tree T3 by creating a new node that is the root of the new tree T3, and has T1, T2 as its left and right subtrees

Thus each step of the algorithm reduces the number of trees in the forest by one; the algorithm terminates when there is only one tree, which is the desired coding trie

The resulting trie is a “full”, but not necessarily “completely filled”, binary tree: every node that is not a leaf has the maximum possible number (2) of children
...and the trie is guaranteed to have the smallest possible average path length (average taken according to probability of occurrence of symbols)

4th step:Huffman’s algorithm pseudocode

0. Determine the count of each symbol in the input message.

1. Create a forest of single-node trees. Each node in the initial forest represents a symbol from the set of possible symbols, and contains the count of that symbol in the message to be coded. Symbols with a count of zero are ignored (consider them to be impossible).
2. Loop while there is more than 1 tree in the forest:
2a. Remove the two trees from the forest that have the lowest count contained in their roots.

2b. Create a new node that will be the root of a new tree. This new tree will have those two trees just removed in step 2a as left and right subtrees. The count in the root of this new tree will be the sum of the counts in the roots of its subtrees. Label the edge from this new root to its left subtree “1”, and label the edge to its right subtree “0”.

2c. Insert this new tree in the forest, and go to 2.
3. Return the one tree in the forest as the Huffman code tree.
